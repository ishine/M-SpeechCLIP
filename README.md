# M-SpeechCLIP
Implementation of the M-SpeechCLIP model, introduced in the paper ["M-SpeechCLIP: Leveraging Large-Scale, Pre-Trained Models for Multilingual Speech to Image Retrieval"](https://arxiv.org/abs/2211.01180)

## Environment Set-Up
The primary requirements for this project are [PyTorch](https://pytorch.org/get-started/locally/) and [CLIP](https://github.com/openai/CLIP).
You can get the exact versions we used by calling:
`pip install -r requirements.txt`

## Data Preparation
TODO: editting some of the files to make changing paths easier
